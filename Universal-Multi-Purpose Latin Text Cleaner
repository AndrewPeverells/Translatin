### 

CAREFUL!

To run this script, you'll need a very powerful computer!

###

import re
import nltk
import cltk
from cltk.tokenize.word import WordTokenizer
from nltk.tokenize.punkt import PunktLanguageVars

# -- PUT YOUR TEXT HERE --
corpus = ""
text = corpus

# CLTK tokenizer

word_tokenizer = WordTokenizer('latin')
word_tokenizer.tokenize(text.lower())

p = PunktLanguageVars()
tokens = p.word_tokenize(sentence)

# General cleaning

class MultiReplacer(object):

    def __init__(self):
        patterns = [(r'v', 'u'), ('V', 'U'), (r'j', 'i'), (r'J', 'I'), (r'uors', 'uers'), (r",", " "), (r"-", ""),
                   (r'0', ''), (r"1", ""), (r"2", ""), (r"3", ""), (r"4", ""), (r"5", ""), (r"6", ""), (r"7", ""), (r"8", ""), (r"9", ""),
                   (r"<", " "), (r">", " "), (r'"', " "), (r"=", " "), (r"^", " "), (r"~", " "),
                   (r',', ' '), (r":", " "), (r";", " "), (r"!", " "), (r"’", " "), (r"-", " "), (r"—", " "),
                   (r'  ', ' '), (r'I.', ''), (r'II.', ''),(r'III.', ''),(r'IV.', ''), (r'V.', ''), (r'ii', ''),
                   (r'iii', ''), (r'iv', ''), (r'v', ''), (r'vi', ''), (r'vii', '')]
        self.patterns = \
            [(re.compile(regex), repl) for (regex, repl) in patterns]

    def clean(self, text):
        for (pattern, repl) in self.patterns:
            text = re.subn(pattern, repl, text)[0]
        return text

j = MultiReplacer()
replaced_text = j.clean(sentence.replace('.', '').replace('?', '').replace('(', '').replace(')', '').replace('[', '').replace(']', '').lower()) ##so che non ha senso, ho dovuto splittare qui alcuni replace perché nella big funzione sopra non me li prendeva per qualche motivo

# Archaisms handler

words = replaced_text

words_split = words.split()

for i, word in enumerate(words_split):
    if words_split[i].endswith("om", -2):       
        words_split[i] = words_split[i].replace("om", "um") ##di base qui mi servirebbe che le parole che finiscono in -om vengano trasformate in -um / da -umum a -imum /  da -ai ad -ae. non sono riuscito a dargli una regola generale da applicare a tutto il testo perché faccio pena con le regex, quindi ho scritto le singole sostituzioni manualmente poco sotto

words_arch = ' '.join(words_split).replace("optumum", "optimum").replace("maxumum", "maximum").replace("familiai", "familiae").replace("quom", "cum")

# Enclitics handler: exceptions database

enclitics = ["que", "n", "ne", "ue", "ve", "st"] ##un'enclitica è una parola che si appoggia alla fine di un'altra (tipo "amandoti" > amando te). mi serve che vengano tutte splittate, quando dunque una parola finisce per "que", "n", "ne", "ue", "ve", "st", TRANNE nei casi elencati da questa mia listona custom, che costituisce le eccezioni per il latino.

que_exceptions = []
n_exceptions = []
ne_exceptions = []
ue_exceptions = []
ve_exceptions = []
st_exceptions = []

# quisque / quique
que_exceptions += [
    "quisque",
    "quidque",
    "quicque",
    "quodque",
    "cuiusque",
    "cuique",
    "quemque",
    "quamque",
    "quoque",
    "quaque",
    "quique",
    "quaeque",
    "quorumque",
    "quarumque",
    "quibusque",
    "quosque",
    "quasque",
]

# uterque
que_exceptions += [
    "uterque",
    "utraque",
    "utrumque",
    "utriusque",
    "utrique",
    "utrumque",
    "utramque",
    "utroque",
    "utraque",
    "utrique",
    "utraeque",
    "utrorumque",
    "utrarumque",
    "utrisque",
    "utrosque",
    "utrasque",
]

# quiscumque
que_exceptions += [
    "quicumque",
    "quidcumque",
    "quodcumque",
    "cuiuscumque",
    "cuicumque",
    "quemcumque",
    "quamcumque",
    "quocumque",
    "quacumque",
    "quicumque",
    "quaecumque",
    "quorumcumque",
    "quarumcumque",
    "quibuscumque",
    "quoscumque",
    "quascumque",
]

# unuscumque
que_exceptions += [
    "unusquisque",
    "unaquaeque",
    "unumquodque",
    "unumquidque",
    "uniuscuiusque",
    "unicuique",
    "unumquemque",
    "unamquamque",
    "unoquoque",
    "unaquaque",
]

# plerusque
que_exceptions += [
    "plerusque",
    "pleraque",
    "plerumque",
    "plerique",
    "pleraeque",
    "pleroque",
    "pleramque",
    "plerorumque",
    "plerarumque",
    "plerisque",
    "plerosque",
    "plerasque",
]

# misc
que_exceptions += [
    "absque",
    "abusque",
    "adaeque",
    "adusque",
    "aeque",
    "antique",
    "atque",
    "circumundique",
    "conseque",
    "cumque",
    "cunque",
    "denique",
    "deque",
    "donique",
    "hucusque",
    "inique",
    "inseque",
    "itaque",
    "longinque",
    "namque",
    "neque",
    "oblique",
    "peraeque",
    "praecoque",
    "propinque",
    "qualiscumque",
    "quandocumque",
    "quandoque",
    "quantuluscumque",
    "quantumcumque",
    "quantuscumque",
    "quinque",
    "quocumque",
    "quomodocumque",
    "quomque",
    "quotacumque",
    "quotcumque",
    "quotienscumque",
    "quotiensque",
    "quotusquisque",
    "quousque",
    "relinque",
    "simulatque",
    "torque",
    "ubicumque",
    "ubique",
    "undecumque",
    "undique",
    "usque",
    "usquequaque",
    "utcumque",
    "utercumque",
    "utique",
    "utrimque",
    "utrique",
    "utriusque",
    "utrobique",
    "utrubique",
]

ne_exceptions += [
    "absone",
    "acharne",
    "acrisione",
    "acumine",
    "adhucine",
    "adsuetudine",
    "aeetine",
    "aeschynomene",
    "aesone",
    "agamemnone",
    "agmine",
    "albane",
    "alcyone",
    "almone",
    "alsine",
    "amasene",
    "ambitione",
    "amne",
    "amoene",
    "amymone",
    "anadyomene",
    "andrachne",
    "anemone",
    "aniene",
    "anne",
    "antigone",
    "aparine",
    "apolline",
    "aquilone",
    "arachne",
    "arne",
    "arundine",
    "ascanione",
    "asiane",
    "asine",
    "aspargine",
    "babylone",
    "barine",
    "bellone",
    "belone",
    "bene",
    "benigne",
    "bipenne",
    "bizone",
    "bone",
    "bubone",
    "bulbine",
    "cacumine",
    "caligine",
    "calymne",
    "cane",
    "carcine",
    "cardine",
    "carmine",
    "catacecaumene",
    "catone",
    "cerne",
    "certamine",
    "chalbane",
    "chamaedaphne",
    "chamaemyrsine",
    "chaone",
    "chione",
    "christiane",
    "clymene",
    "cognomine",
    "commagene",
    "commune",
    "compone",
    "concinne",
    "condicione",
    "condigne",
    "cone",
    "confine",
    "consone",
    "corone",
    "crastine",
    "crepidine",
    "crimine",
    "crine",
    "culmine",
    "cupidine",
    "cyane",
    "cydne",
    "cyllene",
    "cyrene",
    "daphne",
    "depone",
    "desine",
    "dicione",
    "digne",
    "dine",
    "dione",
    "discrimine",
    "diutine",
    "dracone",
    "dulcedine",
    "elatine",
    "elephantine",
    "elleborine",
    "epidamne",
    "erigone",
    "euadne",
    "euphrone",
    "euphrosyne",
    "examine",
    "faune",
    "femine",
    "feminine",
    "ferrugine",
    "fine",
    "flamine",
    "flumine",
    "formidine",
    "fragmine",
    "fraterne",
    "fulmine",
    "fune",
    "germane",
    "germine",
    "geryone",
    "gorgone",
    "gramine",
    "grandine",
    "haecine",
    "halcyone",
    "hammone",
    "harundine",
    "hedone",
    "helene",
    "helxine",
    "hermione",
    "heroine",
    "hesione",
    "hicine",
    "hicne",
    "hierabotane",
    "hippocrene",
    "hispane",
    "hodierne",
    "homine",
    "hominesne",
    "hortamine",
    "hucine",
    "humane",
    "hunccine",
    "huncine",
    "iasione",
    "iasone",
    "igne",
    "imagine",
    "immane",
    "immune",
    "impoene",
    "impone",
    "importune",
    "impune",
    "inane",
    "inconcinne",
    "indagine",
    "indigne",
    "inferne",
    "inguine",
    "inhumane",
    "inpone",
    "inpune",
    "insane",
    "insigne",
    "inurbane",
    "ismene",
    "istucine",
    "itone",
    "iuuene",
    "karthagine",
    "labiene",
    "lacedaemone",
    "lanugine",
    "latine",
    "legione",
    "lene",
    "lenone",
    "libidine",
    "limine",
    "limone",
    "lumine",
    "magne",
    "maligne",
    "mane",
    "margine",
    "marone",
    "masculine",
    "matutine",
    "medicamine",
    "melpomene",
    "memnone",
    "mesene",
    "messene",
    "misene",
    "mitylene",
    "mnemosyne",
    "moderamine",
    "moene",
    "mone",
    "mortaline",
    "mucrone",
    "munimine",
    "myrmidone",
    "mytilene",
    "ne",
    "necne",
    "neptune",
    "nequene",
    "nerine",
    "nocturne",
    "nomine",
    "nonne",
    "nullane",
    "numine",
    "nuncine",
    "nyctimene",
    "obscene",
    "obsidione",
    "oenone",
    "omine",
    "omne",
    "oppone",
    "opportune",
    "ordine",
    "origine",
    "orphne",
    "oxymyrsine",
    "paene",
    "pallene",
    "pane",
    "paraetacene",
    "patalene",
    "pectine",
    "pelagine",
    "pellene",
    "pene",
    "perbene",
    "perbenigne",
    "peremne",
    "perenne",
    "perindigne",
    "peropportune",
    "persephone",
    "phryne",
    "pirene",
    "pitane",
    "plane",
    "pleione",
    "plene",
    "pone",
    "praefiscine",
    "prasiane",
    "priene",
    "priuigne",
    "procne",
    "proditione",
    "progne",
    "prone",
    "propone",
    "pulmone",
    "pylene",
    "pyrene",
    "pythone",
    "ratione",
    "regione",
    "religione",
    "remane",
    "retine",
    "rhene",
    "rhododaphne",
    "robigine",
    "romane",
    "roxane",
    "rubigine",
    "sabine",
    "sane",
    "sanguine",
    "saturne",
    "seditione",
    "segne",
    "selene",
    "semine",
    "semiplene",
    "sene",
    "sepone",
    "serene",
    "sermone",
    "serrane",
    "siccine",
    "sicine",
    "sine",
    "sithone",
    "solane",
    "sollemne",
    "somne",
    "sophene",
    "sperne",
    "spiramine",
    "stamine",
    "statione",
    "stephane",
    "sterne",
    "stramine",
    "subpone",
    "subtegmine",
    "subtemine",
    "sulmone",
    "superne",
    "supine",
    "suppone",
    "susiane",
    "syene",
    "tantane",
    "tantine",
    "taprobane",
    "tegmine",
    "telamone",
    "temne",
    "temone",
    "tene",
    "testudine",
    "theophane",
    "therone",
    "thyone",
    "tiberine",
    "tibicine",
    "tiburne",
    "tirone",
    "tisiphone",
    "torone",
    "transitione",
    "troiane",
    "turbine",
    "turne",
    "tyrrhene",
    "uane",
    "uelamine",
    "uertigine",
    "uesane",
    "uimine",
    "uirgine",
    "umbone",
    "unguine",
    "uolumine",
    "uoragine",
    "urbane",
    "uulcane",
    "zone",
]

n_exceptions += [
    "aenean",
    "agmen",
    "alioquin",
    "an",
    "attamen",
    "cacumen",
    "carmen",
    "certamen",
    "clymenen",
    "cognomen",
    "crimen",
    "culmen",
    "dein",
    "deucalion",
    "discrimen",
    "en",
    "epitheton",
    "erinyn",
    "exin",
    "flumen",
    "forsan",
    "forsitan",
    "fulmen",
    "gramen",
    "hymen",
    "iason",
    "in",
    "limen",
    "liquamen",
    "lumen",
    "nomen",
    "non",
    "numen",
    "omen",
    "orion",
    "paean",
    "pan",
    "pelion",
    "phaethon",
    "python",
    "quin",
    "semen",
    "sin",
    "specimen",
    "tamen",
    "themin",
    "titan",
]

ue_exceptions += [
    "agaue",
    "ambigue",
    "assidue",
    "aue",
    "boue",
    "breue",
    "calue",
    "caue",
    "ciue",
    "congrue",
    "contigue",
    "continue",
    "curue",
    "exigue",
    "exolue",
    "exue",
    "fatue",
    "faue",
    "fue",
    "furtiue",
    "gradiue",
    "graue",
    "ignaue",
    "incongrue",
    "ingenue",
    "innocue",
    "ioue",
    "lasciue",
    "leue",
    "moue",
    "mutue",
    "naue",
    "neue",
    "niue",
    "perexigue",
    "perspicue",
    "pingue",
    "praecipue",
    "praegraue",
    "prospicue",
    "proterue",
    "remoue",
    "resolue",
    "saeue",
    "salue",
    "siue",
    "solue",
    "strenue",
    "sue",
    "summoue",
    "superflue",
    "supplicue",
    "tenue",
    "uiue",
    "ungue",
    "uoue",
]

ve_exceptions += [
    "agave",
    "ave",
    "bove",
    "breve",
    "calve",
    "cave",
    "cive",
    "curve",
    "fave",
    "furtive",
    "gradive",
    "grave",
    "ignave",
    "iove",
    "lascive",
    "leve",
    "move",
    "nave",
    "neve",
    "nive",
    "praegrave",
    "promiscue",
    "prospicve",
    "proterve",
    "remove",
    "resolve",
    "saeve",
    "salve",
    "sive",
    "solve",
    "summove",
    "vive",
    "vove",
]

st_exceptions += [
    "abest",
    "adest",
    "ast",
    "deest",
    "est",
    "inest",
    "interest",
    "post",
    "potest",
    "prodest",
    "subest",
    "superest",
]

latin_exceptions = list(
    set(
        que_exceptions
        + ne_exceptions
        + n_exceptions
        + ue_exceptions
        + ve_exceptions
        + st_exceptions
    )
)

exceptions = list(set(enclitics + latin_exceptions))

latin_replacements = [
    (r"\bmecum\b", "cum me"),
    (r"\btecum\b", "cum te"),
    (r"\bsecum\b", "cum se"),
    (r"\bnobiscum\b", "cum nobis"),
    (r"\bvobiscum\b", "cum vobis"),
    (r"\buobiscum\b", "cum uobis"),
    (r"\bquocum\b", "cum quo"),
    (r"\bquacum\b", "cum qua"),
    (r"\bquicum\b", "cum qui"),
    (r"\bquibuscum\b", "cum quibus"),
    (r"\bsodes\b", "si audes"),
    (r"\bsatin\b", "satis ne"),
    (r"\bscin\b", "scis ne"),
    (r"\bsultis\b", "si vultis"),
    (r"\bsimilist\b", "similis est"),
    (r"\bqualist\b", "qualis est"),
]

# Enclitics handler  ##qui l'actual codice che dovrebbe splittare le enclitiche, l'ho preso e modificato da CLTK

clean_sw = []

for token in words_arch:
    is_enclitic = False
    if token.lower() not in exceptions:
        for enclitic in enclitics:
            if token.endswith(enclitic):
                if enclitic == "n":
                    clean_sw += [token[: -len(enclitic)]] + ["-ne"]
                elif enclitic == "st":
                    if token.endswith("ust"):
                        clean_sw += [token[: -len(enclitic) + 1]] + [
                            "est"
                        ]
                    else:
                        clean_sw += [token[: -len(enclitic)]] + ["est"]
                else:
                    clean_sw += [token[: -len(enclitic)]] + [
                        "-" + enclitic
                    ]
                is_enclitic = True
                break
    if not is_enclitic:
        clean_sw.append(token)

for sent in clean_sw:
            new_clean_sw = word_tokenizer.tokenize(words_arch.lower())
            if sent:
                if new_clean_sw[0].endswith("que"):
                    if new_clean_sw[0].lower() not in exceptions:
                        temp = [new_clean_sw[0][:-2], "-ne"]
                        new_clean_sw = temp + new_clean_sw[1:]
                if new_clean_sw[-1].endswith("."):
                    final_word = new_clean_sw[-1][:-1]
                    del new_clean_sw[-1]
                    new_clean_sw += [final_word, "."]

                for token in new_clean_sw:
                    tokens.append(token)

clean_text = new_clean_sw

# Cleaning stopwords  ##e qui i dolori. mi serve semplicemente che tutte queste parole (che tra l'altro dovrebbero essere anche aggiornate) vengano tolte dal testo. per qualche motivo strano ci mette letteralmente ore, ma non dovrebbe essere così tosta...

stopwords = [
    "a",
    "ab",
    "que",
    "ne",
    "ue",
    "ac",
    "ad",
    "adhic",
    "alius",
    "alia",
    "aliud",
    "alicuius",
    "alicui",
    "aliquem",
    "aliquam",
    "aliquo",
    "aliqua",
    "aliqui",
    "aliquae",
    "aliquorum",
    "aliquarum",
    "aliquas",
    "aliquos",
    "aliquis",
    "an",
    "ante",
    "apud",
    "at",
    "atque",
    "ero",
    "eris", 
    "erit", 
    "erimus",
    "eritis", 
    "erunt",
    "eram",
    "eras",
    "erat",
    "eramus",
    "eratis",
    "erant",
    "sim",
    "sis",
    "sit",
    "simus",
    "sitis",
    "sint",
    "essem",
    "esses",
    "esset",
    "essemus",
    "essetis",
    "essent",
    "aut",
    "autem",
    "contra",
    "cui",
    "cum",
    "cur",
    "de",
    "deinde",
    "dum",
    "ea",
    "eadem",
    "eodem",
    "isdem",
    "iisdem",
    "eosdem",
    "easdem",
    "ego",
    "enim",
    "ergo",
    "eum",
    "es",
    "est",
    "estis",
    "et",
    "etiam",
    "etsi",
    "ex",
    "fore",
    "fui",
    "fuisti",
    "fuit",
    "fuimus",
    "fuistis",
    "fuerunt",
    "fio",
    "fis",
    "fit",
    "fiunt",
    "haud",
    "hic",
    "hinc",
    "hoc",
    "haec",
    "hanc",
    "horum",
    "harum",
    "huic",
    "hi",
    "hae",
    "iam",
    "id",
    "idem",
    "igitur",
    "ille",
    "illis",
    "illi",
    "illae",
    "illos",
    "illa",
    "illud",
    "illum",
    "illo",
    "illae",
    "illorum",
    "illarum",
    "illic",
    "illac",
    "illanc",
    "in",
    "infra",
    "inter",
    "interim",
    "ipse",
    "is",
    "ita",
    "mi",
    "mihi",
    "me",
    "meus",
    "mea",
    "meum",
    "mei",
    "meo",
    "mea",
    "meae",
    "meorum",
    "mearum",
    "meis",
    "meos",
    "meas",
    "magis",
    "modo",
    "mox",
    "nam",
    "ne",
    "nec",
    "necque",
    "neque",
    "nisi",
    "non",
    "nonne",
    "num",
    "nos",
    "nunc",
    "numquam",
    "nunquam",
    "o",
    "ob",
    "per",
    "possum",
    "post",
    "postquam",
    "prae",
    "pro",
    "qua",
    "quae",
    "quam",
    "quom",
    "quorum",
    "quarum"
    "quare",
    "qui",
    "quia",
    "quibus",
    "quabus",
    "quicumque",
    "quidem",
    "quilibet",
    "quin",
    "quis",
    "quisnam",
    "quisquam",
    "quisque",
    "quisquis",
    "quo",
    "quod",
    "quoniam",
    "se",
    "sese",
    "sed",
    "si",
    "simul",
    "sibi"
    "sic",
    "sin",
    "sive",
    "sub",
    "sui",
    "sum",
    "sumus",
    "sunt",
    "super",
    "suus",
    "sua",
    "suum",
    "sui",
    "suo",
    "suam",
    "suum",
    "suae",
    "suorum",
    "suarum",
    "suis",
    "suos",
    "suas",
    "tam",
    "tamen",
    "tibi",
    "trans",
    "tu",
    "te",
    "tui",
    "tua",
    "tuo",
    "tuae",
    "tuorum",
    "tuarum",
    "tuis",
    "tuos",
    "tuas",
    "tum",
    "ubi",
    "uel",
    "uero",
    "unus",
    "ut",
]

def removeStopwords(clean_text, stopwords):
    return [w for w in new_clean_sw if w not in stopwords]

clean_text_no_sw = removeStopwords(new_clean_sw, stopwords)
clean_text_no_sw = ' '.join(clean_text_no_sw)
print(clean_text_no_sw)
